{"ast":null,"code":"import { decodeErrPrefix } from './common.js';\nimport { Type } from './token.js';\nimport { jump, quick } from './jump.js';\nconst defaultDecodeOptions = {\n  strict: false,\n  allowIndefinite: true,\n  allowUndefined: true,\n  allowBigInt: true\n};\n\nclass Tokeniser {\n  constructor(data) {\n    let options = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    this.pos = 0;\n    this.data = data;\n    this.options = options;\n  }\n\n  done() {\n    return this.pos >= this.data.length;\n  }\n\n  next() {\n    const byt = this.data[this.pos];\n    let token = quick[byt];\n\n    if (token === undefined) {\n      const decoder = jump[byt];\n\n      if (!decoder) {\n        throw new Error(`${decodeErrPrefix} no decoder for major type ${byt >>> 5} (byte 0x${byt.toString(16).padStart(2, '0')})`);\n      }\n\n      const minor = byt & 31;\n      token = decoder(this.data, this.pos, minor, this.options);\n    }\n\n    this.pos += token.encodedLength;\n    return token;\n  }\n\n}\n\nconst DONE = Symbol.for('DONE');\nconst BREAK = Symbol.for('BREAK');\n\nfunction tokenToArray(token, tokeniser, options) {\n  const arr = [];\n\n  for (let i = 0; i < token.value; i++) {\n    const value = tokensToObject(tokeniser, options);\n\n    if (value === BREAK) {\n      if (token.value === Infinity) {\n        break;\n      }\n\n      throw new Error(`${decodeErrPrefix} got unexpected break to lengthed array`);\n    }\n\n    if (value === DONE) {\n      throw new Error(`${decodeErrPrefix} found array but not enough entries (got ${i}, expected ${token.value})`);\n    }\n\n    arr[i] = value;\n  }\n\n  return arr;\n}\n\nfunction tokenToMap(token, tokeniser, options) {\n  const useMaps = options.useMaps === true;\n  const obj = useMaps ? undefined : {};\n  const m = useMaps ? new Map() : undefined;\n\n  for (let i = 0; i < token.value; i++) {\n    const key = tokensToObject(tokeniser, options);\n\n    if (key === BREAK) {\n      if (token.value === Infinity) {\n        break;\n      }\n\n      throw new Error(`${decodeErrPrefix} got unexpected break to lengthed map`);\n    }\n\n    if (key === DONE) {\n      throw new Error(`${decodeErrPrefix} found map but not enough entries (got ${i} [no key], expected ${token.value})`);\n    }\n\n    if (useMaps !== true && typeof key !== 'string') {\n      throw new Error(`${decodeErrPrefix} non-string keys not supported (got ${typeof key})`);\n    }\n\n    if (options.rejectDuplicateMapKeys === true) {\n      if (useMaps && m.has(key) || !useMaps && key in obj) {\n        throw new Error(`${decodeErrPrefix} found repeat map key \"${key}\"`);\n      }\n    }\n\n    const value = tokensToObject(tokeniser, options);\n\n    if (value === DONE) {\n      throw new Error(`${decodeErrPrefix} found map but not enough entries (got ${i} [no value], expected ${token.value})`);\n    }\n\n    if (useMaps) {\n      m.set(key, value);\n    } else {\n      obj[key] = value;\n    }\n  }\n\n  return useMaps ? m : obj;\n}\n\nfunction tokensToObject(tokeniser, options) {\n  if (tokeniser.done()) {\n    return DONE;\n  }\n\n  const token = tokeniser.next();\n\n  if (token.type === Type.break) {\n    return BREAK;\n  }\n\n  if (token.type.terminal) {\n    return token.value;\n  }\n\n  if (token.type === Type.array) {\n    return tokenToArray(token, tokeniser, options);\n  }\n\n  if (token.type === Type.map) {\n    return tokenToMap(token, tokeniser, options);\n  }\n\n  if (token.type === Type.tag) {\n    if (options.tags && typeof options.tags[token.value] === 'function') {\n      const tagged = tokensToObject(tokeniser, options);\n      return options.tags[token.value](tagged);\n    }\n\n    throw new Error(`${decodeErrPrefix} tag not supported (${token.value})`);\n  }\n\n  throw new Error('unsupported');\n}\n\nfunction decode(data, options) {\n  if (!(data instanceof Uint8Array)) {\n    throw new Error(`${decodeErrPrefix} data to decode must be a Uint8Array`);\n  }\n\n  options = Object.assign({}, defaultDecodeOptions, options);\n  const tokeniser = options.tokenizer || new Tokeniser(data, options);\n  const decoded = tokensToObject(tokeniser, options);\n\n  if (decoded === DONE) {\n    throw new Error(`${decodeErrPrefix} did not find any content to decode`);\n  }\n\n  if (decoded === BREAK) {\n    throw new Error(`${decodeErrPrefix} got unexpected break`);\n  }\n\n  if (!tokeniser.done()) {\n    throw new Error(`${decodeErrPrefix} too many terminals, data makes no sense`);\n  }\n\n  return decoded;\n}\n\nexport { Tokeniser, tokensToObject, decode };","map":{"version":3,"sources":["/home/legasi/nft-marketplace/node_modules/cborg/esm/lib/decode.js"],"names":["decodeErrPrefix","Type","jump","quick","defaultDecodeOptions","strict","allowIndefinite","allowUndefined","allowBigInt","Tokeniser","constructor","data","options","pos","done","length","next","byt","token","undefined","decoder","Error","toString","padStart","minor","encodedLength","DONE","Symbol","for","BREAK","tokenToArray","tokeniser","arr","i","value","tokensToObject","Infinity","tokenToMap","useMaps","obj","m","Map","key","rejectDuplicateMapKeys","has","set","type","break","terminal","array","map","tag","tags","tagged","decode","Uint8Array","Object","assign","tokenizer","decoded"],"mappings":"AAAA,SAASA,eAAT,QAAgC,aAAhC;AACA,SAASC,IAAT,QAAqB,YAArB;AACA,SACEC,IADF,EAEEC,KAFF,QAGO,WAHP;AAIA,MAAMC,oBAAoB,GAAG;AAC3BC,EAAAA,MAAM,EAAE,KADmB;AAE3BC,EAAAA,eAAe,EAAE,IAFU;AAG3BC,EAAAA,cAAc,EAAE,IAHW;AAI3BC,EAAAA,WAAW,EAAE;AAJc,CAA7B;;AAMA,MAAMC,SAAN,CAAgB;AACdC,EAAAA,WAAW,CAACC,IAAD,EAAqB;AAAA,QAAdC,OAAc,uEAAJ,EAAI;AAC9B,SAAKC,GAAL,GAAW,CAAX;AACA,SAAKF,IAAL,GAAYA,IAAZ;AACA,SAAKC,OAAL,GAAeA,OAAf;AACD;;AACDE,EAAAA,IAAI,GAAG;AACL,WAAO,KAAKD,GAAL,IAAY,KAAKF,IAAL,CAAUI,MAA7B;AACD;;AACDC,EAAAA,IAAI,GAAG;AACL,UAAMC,GAAG,GAAG,KAAKN,IAAL,CAAU,KAAKE,GAAf,CAAZ;AACA,QAAIK,KAAK,GAAGf,KAAK,CAACc,GAAD,CAAjB;;AACA,QAAIC,KAAK,KAAKC,SAAd,EAAyB;AACvB,YAAMC,OAAO,GAAGlB,IAAI,CAACe,GAAD,CAApB;;AACA,UAAI,CAACG,OAAL,EAAc;AACZ,cAAM,IAAIC,KAAJ,CAAW,GAAGrB,eAAiB,8BAA8BiB,GAAG,KAAK,CAAG,YAAYA,GAAG,CAACK,QAAJ,CAAa,EAAb,EAAiBC,QAAjB,CAA0B,CAA1B,EAA6B,GAA7B,CAAmC,GAAvH,CAAN;AACD;;AACD,YAAMC,KAAK,GAAGP,GAAG,GAAG,EAApB;AACAC,MAAAA,KAAK,GAAGE,OAAO,CAAC,KAAKT,IAAN,EAAY,KAAKE,GAAjB,EAAsBW,KAAtB,EAA6B,KAAKZ,OAAlC,CAAf;AACD;;AACD,SAAKC,GAAL,IAAYK,KAAK,CAACO,aAAlB;AACA,WAAOP,KAAP;AACD;;AAtBa;;AAwBhB,MAAMQ,IAAI,GAAGC,MAAM,CAACC,GAAP,CAAW,MAAX,CAAb;AACA,MAAMC,KAAK,GAAGF,MAAM,CAACC,GAAP,CAAW,OAAX,CAAd;;AACA,SAASE,YAAT,CAAsBZ,KAAtB,EAA6Ba,SAA7B,EAAwCnB,OAAxC,EAAiD;AAC/C,QAAMoB,GAAG,GAAG,EAAZ;;AACA,OAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGf,KAAK,CAACgB,KAA1B,EAAiCD,CAAC,EAAlC,EAAsC;AACpC,UAAMC,KAAK,GAAGC,cAAc,CAACJ,SAAD,EAAYnB,OAAZ,CAA5B;;AACA,QAAIsB,KAAK,KAAKL,KAAd,EAAqB;AACnB,UAAIX,KAAK,CAACgB,KAAN,KAAgBE,QAApB,EAA8B;AAC5B;AACD;;AACD,YAAM,IAAIf,KAAJ,CAAW,GAAGrB,eAAiB,yCAA/B,CAAN;AACD;;AACD,QAAIkC,KAAK,KAAKR,IAAd,EAAoB;AAClB,YAAM,IAAIL,KAAJ,CAAW,GAAGrB,eAAiB,4CAA4CiC,CAAG,cAAcf,KAAK,CAACgB,KAAO,GAAzG,CAAN;AACD;;AACDF,IAAAA,GAAG,CAACC,CAAD,CAAH,GAASC,KAAT;AACD;;AACD,SAAOF,GAAP;AACD;;AACD,SAASK,UAAT,CAAoBnB,KAApB,EAA2Ba,SAA3B,EAAsCnB,OAAtC,EAA+C;AAC7C,QAAM0B,OAAO,GAAG1B,OAAO,CAAC0B,OAAR,KAAoB,IAApC;AACA,QAAMC,GAAG,GAAGD,OAAO,GAAGnB,SAAH,GAAe,EAAlC;AACA,QAAMqB,CAAC,GAAGF,OAAO,GAAG,IAAIG,GAAJ,EAAH,GAAetB,SAAhC;;AACA,OAAK,IAAIc,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGf,KAAK,CAACgB,KAA1B,EAAiCD,CAAC,EAAlC,EAAsC;AACpC,UAAMS,GAAG,GAAGP,cAAc,CAACJ,SAAD,EAAYnB,OAAZ,CAA1B;;AACA,QAAI8B,GAAG,KAAKb,KAAZ,EAAmB;AACjB,UAAIX,KAAK,CAACgB,KAAN,KAAgBE,QAApB,EAA8B;AAC5B;AACD;;AACD,YAAM,IAAIf,KAAJ,CAAW,GAAGrB,eAAiB,uCAA/B,CAAN;AACD;;AACD,QAAI0C,GAAG,KAAKhB,IAAZ,EAAkB;AAChB,YAAM,IAAIL,KAAJ,CAAW,GAAGrB,eAAiB,0CAA0CiC,CAAG,uBAAuBf,KAAK,CAACgB,KAAO,GAAhH,CAAN;AACD;;AACD,QAAII,OAAO,KAAK,IAAZ,IAAoB,OAAOI,GAAP,KAAe,QAAvC,EAAiD;AAC/C,YAAM,IAAIrB,KAAJ,CAAW,GAAGrB,eAAiB,uCAAuC,OAAO0C,GAAK,GAAlF,CAAN;AACD;;AACD,QAAI9B,OAAO,CAAC+B,sBAAR,KAAmC,IAAvC,EAA6C;AAC3C,UAAIL,OAAO,IAAIE,CAAC,CAACI,GAAF,CAAMF,GAAN,CAAX,IAAyB,CAACJ,OAAD,IAAYI,GAAG,IAAIH,GAAhD,EAAqD;AACnD,cAAM,IAAIlB,KAAJ,CAAW,GAAGrB,eAAiB,0BAA0B0C,GAAK,GAA9D,CAAN;AACD;AACF;;AACD,UAAMR,KAAK,GAAGC,cAAc,CAACJ,SAAD,EAAYnB,OAAZ,CAA5B;;AACA,QAAIsB,KAAK,KAAKR,IAAd,EAAoB;AAClB,YAAM,IAAIL,KAAJ,CAAW,GAAGrB,eAAiB,0CAA0CiC,CAAG,yBAAyBf,KAAK,CAACgB,KAAO,GAAlH,CAAN;AACD;;AACD,QAAII,OAAJ,EAAa;AACXE,MAAAA,CAAC,CAACK,GAAF,CAAMH,GAAN,EAAWR,KAAX;AACD,KAFD,MAEO;AACLK,MAAAA,GAAG,CAACG,GAAD,CAAH,GAAWR,KAAX;AACD;AACF;;AACD,SAAOI,OAAO,GAAGE,CAAH,GAAOD,GAArB;AACD;;AACD,SAASJ,cAAT,CAAwBJ,SAAxB,EAAmCnB,OAAnC,EAA4C;AAC1C,MAAImB,SAAS,CAACjB,IAAV,EAAJ,EAAsB;AACpB,WAAOY,IAAP;AACD;;AACD,QAAMR,KAAK,GAAGa,SAAS,CAACf,IAAV,EAAd;;AACA,MAAIE,KAAK,CAAC4B,IAAN,KAAe7C,IAAI,CAAC8C,KAAxB,EAA+B;AAC7B,WAAOlB,KAAP;AACD;;AACD,MAAIX,KAAK,CAAC4B,IAAN,CAAWE,QAAf,EAAyB;AACvB,WAAO9B,KAAK,CAACgB,KAAb;AACD;;AACD,MAAIhB,KAAK,CAAC4B,IAAN,KAAe7C,IAAI,CAACgD,KAAxB,EAA+B;AAC7B,WAAOnB,YAAY,CAACZ,KAAD,EAAQa,SAAR,EAAmBnB,OAAnB,CAAnB;AACD;;AACD,MAAIM,KAAK,CAAC4B,IAAN,KAAe7C,IAAI,CAACiD,GAAxB,EAA6B;AAC3B,WAAOb,UAAU,CAACnB,KAAD,EAAQa,SAAR,EAAmBnB,OAAnB,CAAjB;AACD;;AACD,MAAIM,KAAK,CAAC4B,IAAN,KAAe7C,IAAI,CAACkD,GAAxB,EAA6B;AAC3B,QAAIvC,OAAO,CAACwC,IAAR,IAAgB,OAAOxC,OAAO,CAACwC,IAAR,CAAalC,KAAK,CAACgB,KAAnB,CAAP,KAAqC,UAAzD,EAAqE;AACnE,YAAMmB,MAAM,GAAGlB,cAAc,CAACJ,SAAD,EAAYnB,OAAZ,CAA7B;AACA,aAAOA,OAAO,CAACwC,IAAR,CAAalC,KAAK,CAACgB,KAAnB,EAA0BmB,MAA1B,CAAP;AACD;;AACD,UAAM,IAAIhC,KAAJ,CAAW,GAAGrB,eAAiB,uBAAuBkB,KAAK,CAACgB,KAAO,GAAnE,CAAN;AACD;;AACD,QAAM,IAAIb,KAAJ,CAAU,aAAV,CAAN;AACD;;AACD,SAASiC,MAAT,CAAgB3C,IAAhB,EAAsBC,OAAtB,EAA+B;AAC7B,MAAI,EAAED,IAAI,YAAY4C,UAAlB,CAAJ,EAAmC;AACjC,UAAM,IAAIlC,KAAJ,CAAW,GAAGrB,eAAiB,sCAA/B,CAAN;AACD;;AACDY,EAAAA,OAAO,GAAG4C,MAAM,CAACC,MAAP,CAAc,EAAd,EAAkBrD,oBAAlB,EAAwCQ,OAAxC,CAAV;AACA,QAAMmB,SAAS,GAAGnB,OAAO,CAAC8C,SAAR,IAAqB,IAAIjD,SAAJ,CAAcE,IAAd,EAAoBC,OAApB,CAAvC;AACA,QAAM+C,OAAO,GAAGxB,cAAc,CAACJ,SAAD,EAAYnB,OAAZ,CAA9B;;AACA,MAAI+C,OAAO,KAAKjC,IAAhB,EAAsB;AACpB,UAAM,IAAIL,KAAJ,CAAW,GAAGrB,eAAiB,qCAA/B,CAAN;AACD;;AACD,MAAI2D,OAAO,KAAK9B,KAAhB,EAAuB;AACrB,UAAM,IAAIR,KAAJ,CAAW,GAAGrB,eAAiB,uBAA/B,CAAN;AACD;;AACD,MAAI,CAAC+B,SAAS,CAACjB,IAAV,EAAL,EAAuB;AACrB,UAAM,IAAIO,KAAJ,CAAW,GAAGrB,eAAiB,0CAA/B,CAAN;AACD;;AACD,SAAO2D,OAAP;AACD;;AACD,SACElD,SADF,EAEE0B,cAFF,EAGEmB,MAHF","sourcesContent":["import { decodeErrPrefix } from './common.js';\nimport { Type } from './token.js';\nimport {\n  jump,\n  quick\n} from './jump.js';\nconst defaultDecodeOptions = {\n  strict: false,\n  allowIndefinite: true,\n  allowUndefined: true,\n  allowBigInt: true\n};\nclass Tokeniser {\n  constructor(data, options = {}) {\n    this.pos = 0;\n    this.data = data;\n    this.options = options;\n  }\n  done() {\n    return this.pos >= this.data.length;\n  }\n  next() {\n    const byt = this.data[this.pos];\n    let token = quick[byt];\n    if (token === undefined) {\n      const decoder = jump[byt];\n      if (!decoder) {\n        throw new Error(`${ decodeErrPrefix } no decoder for major type ${ byt >>> 5 } (byte 0x${ byt.toString(16).padStart(2, '0') })`);\n      }\n      const minor = byt & 31;\n      token = decoder(this.data, this.pos, minor, this.options);\n    }\n    this.pos += token.encodedLength;\n    return token;\n  }\n}\nconst DONE = Symbol.for('DONE');\nconst BREAK = Symbol.for('BREAK');\nfunction tokenToArray(token, tokeniser, options) {\n  const arr = [];\n  for (let i = 0; i < token.value; i++) {\n    const value = tokensToObject(tokeniser, options);\n    if (value === BREAK) {\n      if (token.value === Infinity) {\n        break;\n      }\n      throw new Error(`${ decodeErrPrefix } got unexpected break to lengthed array`);\n    }\n    if (value === DONE) {\n      throw new Error(`${ decodeErrPrefix } found array but not enough entries (got ${ i }, expected ${ token.value })`);\n    }\n    arr[i] = value;\n  }\n  return arr;\n}\nfunction tokenToMap(token, tokeniser, options) {\n  const useMaps = options.useMaps === true;\n  const obj = useMaps ? undefined : {};\n  const m = useMaps ? new Map() : undefined;\n  for (let i = 0; i < token.value; i++) {\n    const key = tokensToObject(tokeniser, options);\n    if (key === BREAK) {\n      if (token.value === Infinity) {\n        break;\n      }\n      throw new Error(`${ decodeErrPrefix } got unexpected break to lengthed map`);\n    }\n    if (key === DONE) {\n      throw new Error(`${ decodeErrPrefix } found map but not enough entries (got ${ i } [no key], expected ${ token.value })`);\n    }\n    if (useMaps !== true && typeof key !== 'string') {\n      throw new Error(`${ decodeErrPrefix } non-string keys not supported (got ${ typeof key })`);\n    }\n    if (options.rejectDuplicateMapKeys === true) {\n      if (useMaps && m.has(key) || !useMaps && key in obj) {\n        throw new Error(`${ decodeErrPrefix } found repeat map key \"${ key }\"`);\n      }\n    }\n    const value = tokensToObject(tokeniser, options);\n    if (value === DONE) {\n      throw new Error(`${ decodeErrPrefix } found map but not enough entries (got ${ i } [no value], expected ${ token.value })`);\n    }\n    if (useMaps) {\n      m.set(key, value);\n    } else {\n      obj[key] = value;\n    }\n  }\n  return useMaps ? m : obj;\n}\nfunction tokensToObject(tokeniser, options) {\n  if (tokeniser.done()) {\n    return DONE;\n  }\n  const token = tokeniser.next();\n  if (token.type === Type.break) {\n    return BREAK;\n  }\n  if (token.type.terminal) {\n    return token.value;\n  }\n  if (token.type === Type.array) {\n    return tokenToArray(token, tokeniser, options);\n  }\n  if (token.type === Type.map) {\n    return tokenToMap(token, tokeniser, options);\n  }\n  if (token.type === Type.tag) {\n    if (options.tags && typeof options.tags[token.value] === 'function') {\n      const tagged = tokensToObject(tokeniser, options);\n      return options.tags[token.value](tagged);\n    }\n    throw new Error(`${ decodeErrPrefix } tag not supported (${ token.value })`);\n  }\n  throw new Error('unsupported');\n}\nfunction decode(data, options) {\n  if (!(data instanceof Uint8Array)) {\n    throw new Error(`${ decodeErrPrefix } data to decode must be a Uint8Array`);\n  }\n  options = Object.assign({}, defaultDecodeOptions, options);\n  const tokeniser = options.tokenizer || new Tokeniser(data, options);\n  const decoded = tokensToObject(tokeniser, options);\n  if (decoded === DONE) {\n    throw new Error(`${ decodeErrPrefix } did not find any content to decode`);\n  }\n  if (decoded === BREAK) {\n    throw new Error(`${ decodeErrPrefix } got unexpected break`);\n  }\n  if (!tokeniser.done()) {\n    throw new Error(`${ decodeErrPrefix } too many terminals, data makes no sense`);\n  }\n  return decoded;\n}\nexport {\n  Tokeniser,\n  tokensToObject,\n  decode\n};"]},"metadata":{},"sourceType":"module"}